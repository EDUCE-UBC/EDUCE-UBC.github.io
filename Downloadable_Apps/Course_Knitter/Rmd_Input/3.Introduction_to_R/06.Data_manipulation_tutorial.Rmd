---
title: "Data manipulation tutorial"
author: "Dr. Kim Dill-McFarland, U. of British Columbia"
subtitle: "Experiential Data science for Undergraduate Cross-disciplinary Education"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
editor_options: 
  chunk_output_type: console
urlcolor: blue
---
# Data manipulation tutorial
## Learning objectives

- Load tabular data using the [tidyverse](https://www.tidyverse.org/)
- Subset and clean data in `dplyr` (filter, select, rename, arrange, mutate)
- Summarize data in `dplyr` (group_by, summarize)
- Transform data frames using `tidyr` (gather, spread) and `dplyr` (*_join)
- Link multiple tidyverse functions using pipes `%>%`
    
## Setup
If you would like to follow along, open a new RStudio session and create a Project. Download the data file `Saanich_Data.csv` using the following command in RStudio. If you would like to save code/notes, also start a new R script.

```{r}
write.csv(
  read.csv("https://raw.githubusercontent.com/EDUCE-UBC/workshop_data/master/Saanich_Data.csv"),
  "Saanich_Data.csv", row.names=FALSE)
```

Not sure what a Project is? Be sure to include the "RStudio tutorial" in your materials!

If you would like to learn more about Saanich Inlet and these data, checkout [our description](https://educe-ubc.github.io/about_data.html).

## The R tidyverse
Base R gets the job done when it comes to data manipulation, but it is computationally slow and easily becomes difficult to read. For example, this code take data in `dat` and selects several variables (columns) as well as filters to rows where oxygen data is present (not NA).

```{r eval=FALSE}
dat[apply(!is.na(dat[,"WS_O2"]), 1, any), 
        c("Cruise", "Date", "Depth", "WS_O2", "WS_NO3", "WS_H2S")]
```

**There is a better way!**

Currently, the most popular alternative for data wrangling is the package `dplyr` in the [tidyverse](https://www.tidyverse.org/). This package is so good at what it does, and integrates so well with other popular tools like `ggplot2`, that it has rapidly become the de-facto standard and it is what we will focus on in this tutorial. 

Compared to base R, `dplyr` code is much more readable because all operations are based on using `dplyr` functions or _verbs_ (select, filter, mutate...) rather than base R's more difficult to read indexing system (brackets, parentheses...). 

Typical data wrangling tasks in `dplyr`:

- `filter` out a subset of observations (rows)
- `select` a subset of variables (columns)
- `rename` variables
- `arrange` the observations by sorting variable(s) in ascending or descending order
- `mutate` all values of a variable (apply a transformation)

Each verb works similarly:

- input data frame in the first argument
- other arguments can refer to variables as if they were local objects
- output is another data frame

### Install `tidyverse`
In order to use the functions in the tidyverse like those in the `dplyr` package, you must first install these packages. (More information available in our 'RStudio tutorial'.)

```{r eval=FALSE}
install.packages("tidyverse")
```

*Please note that if you have __R v3.3 or older__, you may not be able to install `tidyverse`. In this case, you need to separately install each package within the tidyverse. This includes:* `readr`, `tibble`, `dplyr`, `tidyr`, `stringr`, `ggplot2`, `purr`, `forcats`

### Load `tidyverse`
Installing a package downloads the relevant files onto your computer, but it does not allow R/RStudio to access the functions therein. Thus, you must load packages into RStudio using the `library()` function.

This may seem tedious but it is necessary every time you open a new RStudio session. Otherwise, the program would load every package you had ever downloaded every time it is opened. With 10s of 1000s of packages out there, this would significantly slow down RStudio. Thus, only load the packages you will need for a given project.

For this tutorial, you will need:

**R v3.4 or newer**

```{r eval=FALSE}
library(tidyverse)
```

**R v3.3 or older**

```{r message=FALSE}
library(readr)
library(dplyr)
library(tidyr)
```

## Load data
### Read in tabular data
To start analyzing data, we need to read it into RStudio. There a number of functions to do this; we will start with the most generic one in the tidyverse, `read_delim()`. As with all R functions, you specify the function name and then enclose any specific parameters or arguments within parentheses.

```{r eval=FALSE}
Function(arugument1=..., argument2=..., ...)
```

For example, we load the file `Saanich_Data.csv` like so. 

```{r error=TRUE}
read_delim(file="Saanich_Data.csv")
```

Here, we've told R our `file`. Since we are using a Project, R looks for this file in the Project directory. If we wanted to specify a file not in our Project folder, we would need to provide the full path like if the file were on my computer's Desktop.

```{r eval=FALSE, error=TRUE}
read_delim(file="/Users/kim/Desktop/Saanich_Data.csv")
```

In either case, we get an **Error**, which means that the function did not complete. This occurs because we did not provide `read_delim` with enough information to successfully read in these data.

This is in contrast to a **warning**, which means that the function completed but there were one or more issues.

### Add arugments
We can specify other features of the data using additional arguments in the `read_delim()` function. In the case of these data, we want to use:

* delim: tells R that the data are comma-delimited `","`. If you had a tab-delimited file, you would use `sep="\t"`.

```{r}
read_delim(file="Saanich_Data.csv", delim=",")
```

With these added arguments, we see that our data are now read in and correctly formatted in multiple columns with column (variable) names. Note that the tidyverse has automatically detected and formatted a column in these data that appear to be dates.

### Argument names and order
You will see in the preceding command that for every argument, we provide the name, `=`, and then the necessary input for that argument. However, if you write arguments in the function's default order, you can leave out the argument name.

```{r results=FALSE}
read_delim("Saanich_Data.csv", ",")
```

Default orders are listed in a function's help page (accessed with `?`). Removing argument names should be done *with caution* as it makes the code less readable and runs the risk of breaking if a function updates or changes in the future. And if you provide arguments out of order, the argument name *must* be provided.

For example, this gives an error because the `delim` argument should be second and instead, we have `TRUE`, which is not an allowable input for `delim`.

```{r error=TRUE}
read_delim("Saanich_Data.csv", TRUE, ",")
```

In contrast, specifying the argument names allows you to write them out of order. We will add the `col_names` argument (which is TRUE by default so this does not alter the function's output) to demonstrate this.

```{r eval=FALSE}
read_delim("Saanich_Data.csv", col_names=TRUE, delim=",")
```

### Other `read_` functions
If you look at the `read_delim` help page, you will see several other similar functions. These allow you to read in data using fewer arguments. For example, `read_csv` reads in a comma-delimited file without the need to specify `delim=","`. 

```{r}
read_csv("Saanich_Data.csv")
```

We will use this version to load in data for the remainder of this tutorial since it is the shortest command.

### Save data to the Environment
All of the examples above simply print the data table into the console. This does not allow us to work further with these data! So, we need to name the data as an object in R so that it is saved in the local Environment and used in subsequent analyses.

We do this by prefacing `read_csv` with the name we want to give the table and either `=` or `<-` to assign that name to whatever `read_csv` reads in.

```{r}
dat = read_csv("Saanich_Data.csv")

#Which is equivalent to

dat <- read_csv("Saanich_Data.csv")
```

Now, we see the `dat` object show up in our Environment in the upper right. If we click on that object in the Environment pane, we can see the table. Row and columns names are in grey boxes and data are in white boxes.

Since we are using an R project, we can close RStudio, save our `RScript` and `RData`, and when we re-open using the `.Rproj` file, our data will still be in the Environment! Try it and see!
    
## Subset and clean data
Our `dat` data frame consists of 1605 rows (observations) and 29 columns (variables). However, there are many manipulations needed to clean-up these data prior to visualization or analysis.

### Conditional statements
A conditional statement queries data and returns TRUE or FALSE based on the statement.

Some common statements include:

* `a == b` for 'a is equal to b'
    - Note that this is different from the single `=` used in functions!
* `a != b` for 'a is not equal to b'
* `a > b` for 'a is greater than b'
* `a >= b` for 'a is greater than or equal to b'
    - Similarly for `<` and `<=`
* `a %in% b` for 'a is in b'
* `is.na()` for 'is missing data'
* `!is.na()` for 'is not missing data'
    - Note that the `!` is often used to be a statement into its negative form such as `!=` and `!is.na()`

### Logical operators
You can string together multiple conditional statements using logical operators. These most commonly include:

* `&` for 'and'
* `|` for 'or'

### Filter
You can use `filter` to select specific rows using conditional statements and logical operators.

Below, we filter the data such that we only retain data with oxygen measurements (not NA). *Remember that using `!` means 'not'.*

```{r}
dat <- filter(dat, !is.na(WS_O2))
```

*Note that this variable is a capital O as in Oxygen, not a 0 as in zero.* Tab-completion can help you here!

### Select

You can use the `select` function to focus on a subset of variables (columns). Let's select the variables that we will need for this tutorial, including:

* cruise #
* date
* depth in kilometers
* oxygen (O~2~) in uM
* nitrate (NO~3~) in uM
* hydrogen sulfide (H~2~S) in uM

```{r}
dat <- select(dat, 
              Cruise, Date, Depth,
              WS_O2, WS_NO3, WS_H2S)
```

*Note that we can spread a single function across several lines as long as the preceding line ends with a comma.*

### Rename
You can use the `rename` function to assign new names to your variables. This can be very useful when your variable names are very long and tedious to type. The setup is always **new_name = old_name**

Here, we remove the "WS_" prefixes from some of the variables and add units.

```{r}
dat <- rename(dat, O2_uM=WS_O2, NO3_uM=WS_NO3, H2S_uM=WS_H2S)
```

This is a good point to touch on best practices for units in data. It is best to either name the units in the variable name, such as O2_uM for oxygen in micromolar, or to create a separate column for units, such as O2 = 3.4 and O2_units = uM. You should *never* place the units in the same variable as the measurement because then you cannot plot the variable as a number!

### Arrange
Arrange reorders rows in ascending order of the provided variables. If we want to instead use descending order, we can add the `desc()` function. For example, let's order the data by hydrogen sulfide (`H2S_uM`).

```{r}
dat <- arrange(dat, H2S_uM)
```

### Mutate
Use `mutate(y=x)` to apply a transformation to some variable x and assign it to the variable name y. As with the rename function, it is always **new_name = old_name**.

Here, we multiply Depth by 1000 to convert its units from kilometers to meters.

```{r}
dat <- mutate(dat, Depth_m=Depth*1000)
```

## Link functions with %>%
Thus far, we have cleaned the data by repeatedly overwriting the `dat` object in R. This means that we may run into issues if we loose our place in the script. For example, if we try to re-run our renaming step, it fails since we've already renamed these variables and the original names no longer exist in `dat`.

```{r error=TRUE}
dat <- rename(dat, O2_uM=WS_O2, NO3_uM=WS_NO3, H2S_uM=WS_H2S)
```

Instead, we can start from our original `dat` object and chain commands together using the `%>%` operator, which is called a pipe. This is done by nesting the functions within one another. In generic terms, this is

`g(f(x))` 

where the data `x` are first processed using the function `f()` and then the resulting data are fed directly into the function `g()`.

In tidyverse syntax using a pipe, the above is equivalent to 

`f(x) %>% g()`

This condenses code and improves readability. Of note, the keyboard shortcut for the pipe in R is Cmd+Shft+M (Mac) or Ctrl+Shft+M (PC).

So, instead of reading in the data, saving as `dat`, and then filtering like so,

```{r eval=FALSE}
dat <- read_csv("Saanich_Data.csv")

dat <- filter(dat, !is.na(WS_O2))
```

We can pipe the data frame read into R with `read_delim` directly into our first cleaning step to `filter` out observations without any oxygen data. Note how we do not input the `dat` data as the first argument of filter anymore. This is because the pipe is providing the data input in lieu of providing it as an explicit argument.

```{r eval=FALSE}
dat <- read_csv("Saanich_Data.csv") %>% 
  filter(!is.na(WS_O2))
```

We can continue to string together functions until all of our previous data cleaning steps (from filter to mutate) are encompassed in 1 piped tidyverse command.

```{r}
dat <- 
  read_csv("Saanich_Data.csv") %>%
  filter(!is.na(WS_O2)) %>% 
  select(Cruise, Date, Depth, WS_O2, WS_NO3, WS_H2S) %>% 
  rename(O2_uM=WS_O2, NO3_uM=WS_NO3, H2S_uM=WS_H2S) %>% 
  arrange(H2S_uM) %>% 
  mutate(Depth_m=Depth*1000)
```

We could even go as far back as the Setup instructions and link the reading of the data from the web into our find tidyverse command.

```{r}
dat <- 
  read_csv("https://raw.githubusercontent.com/EDUCE-UBC/workshop_data/master/Saanich_Data.csv") %>%
  filter(!is.na(WS_O2)) %>% 
  select(Cruise, Date, Depth, WS_O2, WS_NO3, WS_H2S) %>% 
  rename(O2_uM=WS_O2, NO3_uM=WS_NO3, H2S_uM=WS_H2S) %>% 
  arrange(H2S_uM) %>% 
  mutate(Depth_m=Depth*1000)
```

## Summarize data
Another `dplyr` function that effectively utilizes pipes is `summarize` (or `summarise`). This function is handy when we want to calculate descriptive statistics for groups of observations in the data. This is done by first applying the `group_by` verb and then feeding the grouped data into `summarise`. 

For example, we can calculate the mean, standard deviation, and sample size of oxygen concentrations by depth as follows. 

```{r}
dat %>%
  group_by(Depth_m) %>%
  summarise(Mean_O2=mean(O2_uM),
            SD_O2=sd(O2_uM),
            n=n())
```

You can find a list of other common statistical functions in the [stats package](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html) that comes pre-installed with R.

## Transform data frames
### Gather and spread
Tabular data can come in long/narrow/skinny, short/wide/fat, or a combination of these two forms. 

For example, this wide data

```{r, echo=FALSE}
set.seed(123)
wide = data.frame(
  sample_ID = c(1,2,3,4),
  year_2015 = runif(4, 0, 1) %>% round(3), 
  year_2016 = runif(4, 0.2, 1.2) %>% round(3),
  year_2017 = runif(4, 0.5, 1.5) %>% round(3)
)

wide
```

contains the same information as this long data.

```{r echo=FALSE}
gather(wide, key="Year", value="Value", -sample_ID)
```

In their pure forms, wide data have as few rows and as many columns as possible for a given data set (while long data are the opposite - many rows, few columns).

Practically speaking, however, most data are a combination of the two types. For example, the `dat` data are in *partially* wide format. Observe that each variable is given its own column (wide) but time is already gathered into 1 column (long). 

```{r}
dat
```

`gather` and `spread` are two functions from the `tidyr` package that enable easy conversion between wide/long formats. The first will `gather` your wide data into one column containing all of the variable names (the "Key" column) and another column containing all of the values (the "Value" column). The second function will `spread` your long data into the wide data format. 

Here, we gather the three geochemical variables (O2_uM, NO3_uM, H2S_uM) from wide to long format. We'll save this as `dat2` so that we can easily compare the outcome to `dat`.

```{r}
dat2 <- gather(dat, key="Key", value="Value", O2_uM, NO3_uM, H2S_uM)  

dat2
```

We see that our transformed data is indeed much longer (*i.e.* has more rows).

```{r}
dim(dat)

dim(dat2)
```

And that the new Key column contains the 3 variables names that were gathered.

```{r}
unique(dat2$Key)
```

We can then undo this by spreading our data from long to wide format. 

```{r}
dat2 <- spread(dat2, key="Key", value="Value")
dat2
```

So that it is now back to the original shape.

```{r}
dim(dat)

dim(dat2)
```

*Note that you can name the key and value columns anything that you want.* For example, in the above case, we could have named the key and value based on what we know about these data.

```{r}
gather(dat, key="Geochemical", value="uM", O2_uM, NO3_uM, H2S_uM)  
```

You can see several applications of gather/spread in our 'Data visualization tutorial'.

### *_join

Oftentimes you will need to work with data across multiple tables. These tables may function separately or they may need to be combined. If they need to be combined, `dplyr`'s suite of joining functions can be applied. Theses include:

- inner_join(x,y)
    - Keeps rows found in *both* x and y; keeps all columns from *both* x and y
    - Combine all data in x and y, keeping only rows found in both x and y
- semi_join(x,y)
    - Keeps rows found in *both* x and y; keeps all columns from x
    - Filter data in x to include only rows found in both x and y

- anti_join(x,y)
    - Keeps rows from x that are NOT found in y; keeps all columns from x
    - Filter data in x to include only rows NOT also found in y

- left_join(x,y)
    - Keeps all rows from x; keeps all columns from x and y
    - Combine all data in x and y, keeping only rows from x
- right_join(x,y)
    - Keeps all rows from y; keeps all columns from x and y
    - Combine all data in x and y, keeping only rows from y
- full_join(x,y)
    - Keeps all rows and all columns from both x and y
    - Combine all data in x and y, adding NAs where rows do not match

To practice this, we will artificially split up our data into separate data frames for oxygen and nitrate. We will also arrange the data by its geochemical variable (so that their rows are not in the same order) and filter out zeroes (so that not all rows in one are present in the other).

```{r}
dat_O2 <- dat %>% 
  select(Cruise, Date, Depth_m, O2_uM) %>% 
  arrange(O2_uM) %>% 
  filter(O2_uM != 0)
  
dat_NO3 <- dat %>% 
  select(Cruise, Date, Depth_m, NO3_uM) %>% 
  arrange(NO3_uM) %>% 
  filter(NO3_uM != 0)
```

We can see that we now have two data frames of different sizes with rows in differing orders.

```{r}
dat_O2
dat_NO3
```

Thus, we cannot recombine these data by simply pasting 1 table next to the other. Instead, we can use joining.

If we wanted to recreate the original data in `dat`, we would use `full_join` to keep all rows and all columns from both data frames. 

For all of the joining functions, you can either allow R to automatically detect the matching columns

```{r}
full_join(dat_O2, dat_NO3)
```

or explicitly state which columns to compare.

```{r}
full_join(dat_O2, dat_NO3, by=c("Cruise", "Date", "Depth_m"))
```

In either case, we see that we've combined the oxygen and nitrate data back into 1 data frame, though there are now NAs where we deleted 0 data from the original `dat`.

***